# robots.txt for https://www.bnbcc.com.au
# Purpose: Control crawler access, optimize crawl budget, and prepare for LLM/AI integration.

# Allow all major search engines full access to content
User-agent: Googlebot
Allow: /

User-agent: Bingbot
Allow: /

# Block low-value or sensitive areas
User-agent: *
Disallow: /cgi-bin/
Disallow: /admin/
Disallow: /login/
Disallow: /temp/
Disallow: /private/

# Sitemap location
Sitemap: https://www.bnbcc.com.au/sitemap.xml

# --- Optional AI/LLM Crawling Control ---
# Google-Extended is used by Google for AI training (e.g., Bard / Gemini).
# Uncomment the lines below if you wish to OPT OUT of AI data usage.
# User-agent: Google-Extended
# Disallow: /
